# 数据挖掘知识整理2 数据预处理2


## 1数据质量
主要问题有：噪声，利群点，缺失值，重复值

## 2数据预处理的主要任务

* 数据清洗
* 数据整合
* 数据降低
* 数据转换与离散化
  * normalize
  * 概念层次生成  

#### 2.1 数据清洗

* 不完整：缺少属性
* 噪声
* 不一致
* 刻意的数据

##### 清除缺失数据

* 方法1：忽略，通常在类别信息缺失的时候使用，当缺失比例较大时不能使用
* 方法2：人为填补缺失数据（冗长，且不可行）
* 方法3：自动填补
  * 用某个常量填补
  * 用属性均值填补
  * 同一类别这一属性的均值填补
  * 最有可能的值来填补

##### 清楚噪声数据

* bining：排序，装入不同的桶，然后用桶中数据的均值，中位数，范围来平滑
* 回归： 用回归函数来平滑
* 聚集：检测并排除离群点
* 将电脑与人的观察结合在一起

#### 2.2 数据降维

##### 降低维度的策略

* 维度下降（删除认为不重要的属性）：PCA，特征选择，特征创造，小波转换
* 数据下降：回归和log回归模型，直方图，聚集，抽样，数据立方聚集？
* 数据压缩

##### PCA

* 主要思想：
  1. 找到使得数据方差最大的投影
  2. 将数据投射到这个投影上达到降低维度的目的，发现矩阵的特征向量决定了这个空间

* 过程
  1. 数据集构成举证 *M*,每一行代表一个样本，没一列代表一维度的属性
  2. 计算*MT*M*,求它的特征向量x1, x2……,按特征值大小排序，选大的几个构成矩阵
  3. *M*与特征向量构成的矩阵相乘，得到新的数据集，实现降维。

##### 属性子集合选择

* 在属性选择中的启发式搜索
  1. d个属性可能的组合数由2的d次方种
  2. 经典启发式属性选择方法：
     * 方法1：在属性不相关的假设下：通过有意义的测试选择最好的一个属性。
     * 方法2：逐步选择一些最好的属性
     * 方法3：逐步剔除一些最差的属性
     * 方法4：最好属性组合的选择与剔除

##### 创造属性

3种总体方法
1. 特征提取
2. 让数据映射到新的空间
3. 特征构造
    * 组合属性
    * 数据离散化

#### 2.3 降低数量

两大类方法

1. 参数方法： 假设数据符合某种模型，保存模型参数
2. 非参数方法： 直方图，聚集，抽样等

##### 回归分析

1. 线性回归： y = wX + b
2. Multiple regression: Y = b0 + b1 X1 + b2 X
3. log线性回归

##### 非参数方法

1. 直方图分析
2. 聚集（第10章中会具体介绍）
3. 抽样：简单随机抽样，无放回，有放回，分层（解决数据倾斜）

#### 2.4 数据压缩
略

#### 2.5 数据转换

##### 数据转换方法
1. 平滑：去除噪声
2. 属性/特征构造：从已有的属性构造新的属性
3. 数据聚合
4. *标准化*
    * 最大最小标准化：
       * new_v = new_min + (v-min(vs))/(max(vs)-min(vs))(new_max-new_min)
    * z-score标准化:
       1. 标准化后新数据方差为1，均值为0
       2. new_v = (old_v - mean(old_vs))/sd(old_vs)
    * decimal scaling
       * new_v =  old_v / 10 ** s
       * 10 ** s >= max(abs(min(old_vs)), abs(max(old_vs)))   
5. 离散化
    1. bining
        1. 划分方法
       * 等距离划分：根据属性最大最小值等距离划分
       * 等深度划分：划分使得每个桶里数据的数目大体相同
        2. 平滑方法
       * 用均值平滑
       * 用范围平滑（离最大近就最大，离最小近就最小）
    2. Histogram analysis
    3. Clustering analysis
    4. Decision-tree analysis
    5. Correlation (e.g., c2) analysis

#### 2.6 相似性与互异性

* Minkowski Distance：r=1街区距离，r=2欧式距离，r=无穷：向量间差异最大的属性的差异
* 距离矩阵

#####   Binary Vectors之间的距离


* M01 = 属性中p=0,q=1的个数
* M10 = 属性中p=1,q=0的个数
* M00 = 属性中p=0,q=0的个数 
* M11 = 属性中p=1,q=1的个数
* smc（p,q） = (M00 + M11) / 总属性个数
* Jaccard（p,q）= M11 / sum(M01, M10, M11)

##### 余弦相似度

cos(x1, x2) = (x1 dot_product x2)/ (||x1||*||x2||)

##### 扩展的jaccard 系数(Tanimoto)

T(x1, x2) = dot_product(x1, x2) / (||x1||**2 + ||x2||**2 - dot_product(x1, x2))


#### 结合相似性的一般步骤


1. 对每个属性k，计算相似度sk
2. 定义wk，
    * bk = 0,当属性k是binary不对称属性且两个object都为0，或者一个objext
    * bk = 1, otherwise。
3. similarity(x1, x2) = sum([bk * sk for k in range(d)]) / sum([bk for k in range(d)]) 其中d为总属性个数 
4. 可以在计算相似度时给不同的属性加以权重，权重之和为1

#### 2.7相关性


##### Pearson’s Correlation Coefficient

* 计算数值属性之间的相关性
* corr（x, y） = s(x,y) / (s(x)*s(y))
    * 其中 s(x,y) = sum( (xi - mean_x)*(yi - mean_y) for i in range(n)) / (n-1)
    * 标准差s(x) = square( sum( (xi - mean_x) ** 2 for i in range(n)) / (n-1))
* corr(x, y) = 0，则x, y 独立
* corr(x, y) >0，正相关，反之负相关



#### Chi-Square Test
     
检测非数值型（属性取值离散，有限）

* count(x1, y1)：x=x1, y=y1的object个数
* e(i, j): x=x1, y=y1的object的期望个数
    * e(i, j) = count(i, *) * count(*, j) / count
* Chi-Square统计量 = sum([sum([count(xi,xj) - e(i,j) for j in range(n)]) for i in range(m)])
* Chi-Square统计量越大，x,y相关的可能性越大
* (m-1)*(n-1)，在null假设下。。。？




















