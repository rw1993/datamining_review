决策树复习
---
三种划分方法：

1. gini
```python
def gini(node):
    return 1 - sum(label[i].percent**2 for i in node.labels)
    # 100% - 这个节点中每个class的样本所占总体百分比的平方
    # gini系数越小越好
    # cart，sliq等算法使用
    # 划分后的gini系数为子节点的系数的加权和
 ```
 
2. Entropy

```python
def entropy(node):
    return -sum(label[i].percent*log(label[i].percent) for i in node.labels)
    # 越小越好
    # 划分后的熵也为子节点熵的加权和
    #  

def gain(node):
    return entropy(node) - entropy(node.splits())
 
def gainRadio(node):
    return gain(node) / (-(ns[i] /n)*log(ns[i]/n) for in node.sons)
    # ns[i]:划分之后的第i个子节点包含的样本数。
    # n 总体样本数 
    #C4.5 先选gain高的，再从中选gainRadio高的
 ```
 
 3. Misclassification error

```python
def misclassfication_error(node):
    return 1 - max(ns[i] / n for i in node.labels)
    # ns[i]为第i类样本的数量
    # 越小越好
    # 同样用加权和来衡量分裂前后
    
```


泛化误差的衡量

* e(t)这个节点的错误率 （fn+fp）/ (tp+tn+fn+fp)
* e'(t)这个节点的泛化误差 e(t) + 0.5
* e'（T）= e(T) + (N * 0.5)其中N是决策树中叶节点的数量


剪枝：
* pre:预先设定些停止分裂的条件
* post：如果去掉叶节点，使得e'(T)变大，就去掉


























